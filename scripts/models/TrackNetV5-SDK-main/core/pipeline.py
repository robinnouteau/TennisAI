# -*- coding: utf-8 -*-
import cv2
from tqdm import tqdm
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class BallPoint:
    """ä¼ é€å¸¦ä¸Šçš„æ ‡å‡†æ•°æ®åŒ…"""
    x: float = 0.0
    y: float = 0.0
    conf: float = 0.0
    is_detected: bool = False

class TennisPipeline:
    def __init__(self, detector, tracker, visualizer):
        """
        è‡ªåŠ¨åŒ–æµæ°´çº¿è°ƒåº¦ä¸­å¿ƒ
        :param detector: Stage 1 å¤„ç†å™¨ (Detector å®ä¾‹)
        :param tracker: Stage 2 å¤„ç†å™¨ (BaseTracker çš„å­ç±»å®ä¾‹)
        :param visualizer: Stage 3 å¤„ç†å™¨ (Visualizer å®ä¾‹)
        """
        self.detector = detector
        self.tracker = tracker
        self.visualizer = visualizer
        
        # å†…éƒ¨çŠ¶æ€å­˜å‚¨
        self.raw_points: List[BallPoint] = []
        self.refined_dict: Dict[int, BallPoint] = {}

    def run(self, video_path: str, output_dir: str):
        """æ‰§è¡Œå…¨æµç¨‹ï¼šæ£€æµ‹ -> è¿½è¸ª -> æ¸²æŸ“"""
        video_path = Path(video_path)
        output_root = Path(output_dir) / video_path.stem
        output_root.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ Starting Pipeline for: {video_path.name}")

        # --- Stage 1: Detection Pass ---
        # äº§å‡ºåŒ…å« conf çš„åŸå§‹åæ ‡åˆ—è¡¨
        self.raw_points = self._pass1_detection(video_path)

        # --- Stage 2: Tracking Pass ---
        # ç®—æ³•ä»‹å…¥ï¼Œåˆ©ç”¨å…¨å±€ä¿¡æ¯ä¿®å¤ raw_pointsï¼Œäº§å‡ºåæ ‡å­—å…¸
        self.refined_dict = self._pass2_tracking(self.raw_points)

        # --- Stage 3: Rendering Pass ---
        # æ ¹æ®ä¿®å¤åçš„å­—å…¸è¿›è¡ŒäºŒæ¬¡éå†ï¼Œé«˜æ¸…æ¸²æŸ“
        output_video_path = output_root / f"{video_path.stem}_refined.mp4"
        self._pass3_rendering(video_path, str(output_video_path), self.refined_dict)

        print(f"âœ… All stages completed. Result saved to: {output_video_path}")

    def _pass1_detection(self, video_path: Path) -> List[BallPoint]:
        """ç¬¬ä¸€éæ‰«æï¼šGPU å¯†é›†å‹æ£€æµ‹"""
        print("\n[Stage 1/3] Running Neural Inference...")
        # è°ƒç”¨ detector çš„æ¥å£ï¼Œè·å–å…¨è§†é¢‘åŸå§‹åæ ‡
        return self.detector.detect_video(str(video_path))

    def _pass2_tracking(self, raw_points: List[BallPoint]) -> Dict[int, BallPoint]:
        """ç¬¬äºŒéæ‰«æï¼šç®—æ³•çº§è½¨è¿¹ä¼˜åŒ–"""
        print("\n[Stage 2/3] Refining Trajectory with Tracker...")
        # å°†åŸå§‹åˆ—è¡¨ï¼ˆå« confï¼‰äº¤ç»™è¿½è¸ªå™¨è¿›è¡Œé€»è¾‘ä¿®å¤
        return self.tracker.refine(raw_points)

    def _pass3_rendering(self, video_path: Path, output_path: str, refined_dict: Dict[int, BallPoint]):
        """ç¬¬ä¸‰éæ‰«æï¼šI/O å¯†é›†å‹æ¸²æŸ“"""
        print("\n[Stage 3/3] Rendering Final Video...")
        
        cap = cv2.VideoCapture(str(video_path))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        pbar = tqdm(total=total_frames, desc="Rendering")
        
        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # ä»â€˜åæ ‡æ‰‹å†Œâ€™ä¸­æå–å½“å‰å¸§ç»è¿‡ä¿®å¤çš„ BallPoint
            current_point = refined_dict.get(frame_idx, BallPoint(is_detected=False))
            
            # è°ƒç”¨ visualizer æ‰§è¡Œæ¸²æŸ“é€»è¾‘ï¼ˆåŠå¾„è¡°å‡ç­‰ï¼‰
            rendered_frame = self.visualizer.render(frame, current_point)
            
            out.write(rendered_frame)
            frame_idx += 1
            pbar.update(1)
            
        pbar.close()
        cap.release()
        out.release()