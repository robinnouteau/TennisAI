import argparse
import torch
torch.backends.cudnn.benchmark = True
# from torch.optim import lr_scheduler
import torch.optim.lr_scheduler as pt_lr_scheduler
from torch.utils.data import DataLoader
import importlib.util
from pathlib import Path
import sys # ç¡®ä¿å¯¼å…¥ sys
 
#torch.autograd.set_detect_anomaly(True)

# --- 1. å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„â€œå·¥å‚â€çš„å»ºé€ å‡½æ•° ---
# å¯¼å…¥é¡¶å±‚åŒ…ï¼Œå¯¹åº”çš„ __init__.py æ–‡ä»¶ä¼šç¡®ä¿æ‰€æœ‰æ¨¡å—éƒ½å·²æ³¨å†Œ
import models_factory
import datasets_factory
import losses_factory
import optimizers_factory
import metrics_factory
import engine

# å¯¼å…¥æˆ‘ä»¬æœ€ç»ˆçš„â€œæ‰§è¡Œå™¨â€ Runner
from engine.runner import Runner

def load_config_from_path(config_path: str):
    """ä» .py æ–‡ä»¶è·¯å¾„ä¸­åŠ è½½é…ç½®æ¨¡å—ã€‚"""
    config_path = Path(config_path)
    if not config_path.is_file():
        raise FileNotFoundError(f"Config file not found at: {config_path}")
        
    # å°† .py æ–‡ä»¶ä½œä¸ºæ¨¡å—åŠ è½½
    spec = importlib.util.spec_from_file_location(config_path.stem, config_path)
    cfg_module = importlib.util.module_from_spec(spec)
    
    # ã€é‡è¦ã€‘ç¡®ä¿æ¨¡å—å¯ä»¥è¢«å†æ¬¡åŠ è½½ï¼ˆå¦‚æœPythonå·²ç»ç¼“å­˜äº†åŒåæ¨¡å—ï¼‰
    if config_path.stem in sys.modules:
        del sys.modules[config_path.stem]
        
    spec.loader.exec_module(cfg_module)
    return cfg_module

# ===================================================================
# æ­¥éª¤ 1: å°†ä½ åŸæ¥çš„ main() å‡½æ•°é‡æ„ä¸º run_experiment()
# ===================================================================
def run_experiment(config_path_str: str):
    """
    ä½¿ç”¨å•ä¸ªé…ç½®æ–‡ä»¶è¿è¡Œä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒã€‚
    ï¼ˆè¿™å°±æ˜¯ä½ åŸæ¥ main() å‡½æ•°çš„ä¸»ä½“å†…å®¹ï¼‰
    """
    
    try:
        print(f"\n{'='*80}")
        print(f"ğŸš€ [STARTING] å®éªŒ: {config_path_str}")
        print(f"{'='*80}\n")
        
        # --- A. åŠ è½½é…ç½® ---
        # ä¸å†ç¡¬ç¼–ç ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        cfg = load_config_from_path(config_path_str)
        print("âœ… Configuration loaded successfully.")
        
        # --- B. ç¯å¢ƒè®¾ç½® (ç”± Runner å†…éƒ¨å¤„ç†æˆ–åœ¨è¿™é‡Œè®¾ç½®) ---
        # (ä¿æŒä¸å˜)
        
        # --- C. ä½¿ç”¨å·¥å‚æŒ‰å›¾ç´¢éª¥ï¼Œæ„å»ºæ‰€æœ‰ç»„ä»¶ ---
        print("Building components from config...")
        
        # æ„å»ºæ¨¡å‹
        model = models_factory.build_model(cfg.model)
        print("âœ… Model built successfully.")

        # æ„å»ºæ•°æ®é›†
        train_dataset = datasets_factory.build_dataset(cfg.data['train'])
        val_dataset = datasets_factory.build_dataset(cfg.data['val'])
        print("âœ… Datasets built successfully.")
        
        # æ„å»ºæ•°æ®åŠ è½½å™¨ (DataLoader)
        train_loader = DataLoader(
            dataset=train_dataset,
            batch_size=cfg.data['samples_per_gpu'],
            num_workers=cfg.data['workers_per_gpu'],
            shuffle=True,
            pin_memory=True,
            persistent_workers=True
        )
        val_loader = DataLoader(
            dataset=val_dataset,
            batch_size=cfg.data['samples_per_gpu'],
            num_workers=cfg.data['workers_per_gpu'],
            shuffle=False,
            pin_memory=True,
            persistent_workers=True
        )
        print("âœ… DataLoaders built successfully.")

        # æ„å»ºæŸå¤±å‡½æ•°
        criterion = losses_factory.build_loss(cfg.loss)
        print("âœ… Loss function built successfully.")

        # æ„å»ºä¼˜åŒ–å™¨
        optimizer = optimizers_factory.build_optimizer(model, cfg.optimizer)
        print("âœ… Optimizer built successfully.")
        
        # æ„å»ºè¯„ä¼°æŒ‡æ ‡
        metric = metrics_factory.build_metric(cfg.evaluation['metric'])
        print("âœ… Metric built successfully.")

        # æ„å»ºé’©å­ (Hooks)
        hooks = engine.build_hooks(cfg.log_config['hooks'])
        if hasattr(cfg, 'custom_hooks'):
            hooks.extend(engine.build_hooks(cfg.custom_hooks))
        print("âœ… All Hooks built successfully.")

        # æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = None
        if hasattr(cfg, 'lr_config') and cfg.lr_config is not None:
            policy = cfg.lr_config.get('policy', None)
            
            if policy == 'Step':
                if 'step' not in cfg.lr_config:
                    raise ValueError("Step policy requires 'step'(list of milestones in lr_config")
                scheduler = pt_lr_scheduler.MultiStepLR(
                    optimizer,
                    milestones=cfg.lr_config['step'],
                    gamma=cfg.lr_config.get('gamma', 0.1)
                )
                print(f"âœ… LR MultiStepLR scheduler built successfully.")

            elif policy == 'CosineAnnealing':
                scheduler = pt_lr_scheduler.CosineAnnealingLR(
                    optimizer,
                    T_max=cfg.total_epochs - cfg.lr_config.get('warmup_iters', 0),
                    eta_min=cfg.lr_config.get('min_lr', 0)
                )
                print(f"âœ… LR CosineAnnealingLR scheduler built successfully.")
            else:
                print(f"âš ï¸ LR policy '{policy}' is not supported yet or is None. Running with a fixed learning rate.")
                scheduler = None
        else:
            print("â„¹ï¸ No LR scheduler configured. Running with a fixed learning rate.")

        # --- D. å®ä¾‹åŒ–â€œèµ›è½¦æ‰‹â€(Runner) ---
        runner = Runner(
            model=model,
            optimizer=optimizer,
            criterion=criterion,
            metric=metric,
            train_loader=train_loader,
            val_loader=val_loader,
            lr_scheduler=scheduler,
            hooks=hooks,
            cfg=cfg,
            # IsAMP=True
        )
        
        # --- E. å¯åŠ¨è®­ç»ƒï¼---
        runner.run()

        print(f"\n{'-'*80}")
        print(f"âœ… [SUCCESS] å®éªŒ {config_path_str} å®Œæˆï¼")
        print(f"{'-'*80}\n")

    except Exception as e:
        # æ·»åŠ é”™è¯¯å¤„ç†ï¼Œç¡®ä¿ä¸€ä¸ªå®éªŒå¤±è´¥åï¼Œé˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå®éªŒèƒ½ç»§ç»­
        print(f"\n{'!'*80}")
        print(f"âŒ [FAILED] å®éªŒ {config_path_str} é‡åˆ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        print(f"{'!'*80}\n")
    
    # ã€é‡è¦ã€‘æ¸…ç†æ˜¾å­˜ï¼Œä¸ºä¸‹ä¸€ä¸ªå®éªŒåšå‡†å¤‡
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# ===================================================================
# æ­¥éª¤ 2: æ–°å»º main() å‡½æ•°ä½œä¸ºâ€œå®éªŒå¯åŠ¨å™¨â€
# ===================================================================
def main():
    """
    æ–°çš„ä¸»å‡½æ•°ï¼šæ‰«æé…ç½®ã€æ˜¾ç¤ºèœå•å¹¶æŒ‰é¡ºåºè¿è¡Œæ‰€é€‰å®éªŒã€‚
    """
    config_dir = Path('./configs/')
    
    # 1. è‡ªåŠ¨æ‰«æ configs/official ä¸‹çš„æ‰€æœ‰ .py æ–‡ä»¶
    print(f"ğŸ” æ­£åœ¨æ‰«æ {config_dir}...")
    # ä½¿ç”¨ sorted ç¡®ä¿æ¯æ¬¡è¿è¡Œçš„é¡ºåºéƒ½ä¸€æ ·
    config_paths = sorted([p for p in config_dir.glob('**/*.py') if p.name != '__init__.py'])
    
    if not config_paths:
        print(f"âŒ é”™è¯¯: åœ¨ {config_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• .py é…ç½®æ–‡ä»¶ã€‚")
        return

    # 2. å±•ç¤ºä¸€ä¸ªèœå•
    print("\n--- ğŸ“œ å¯ç”¨çš„é…ç½®æ–‡ä»¶ ---")
    for i, path in enumerate(config_paths, 1):
        # ä½¿ç”¨ :>2 æ¥å³å¯¹é½åºå·ï¼Œä½¿å…¶æ›´ç¾è§‚
        print(f"  {i:>2}: {path.name}")
    print("---------------------------\n")

    # 3. è®©ä½ è¾“å…¥æƒ³è¿è¡Œçš„åºå·
    while True:
        selection_str = input("ğŸ‘‰ è¯·è¾“å…¥è¦è¿è¡Œçš„é…ç½®åºå· (ç”¨ç©ºæ ¼éš”å¼€, å¦‚ '1 3 2'): ")
        selected_indices = selection_str.split()
        
        experiments_to_run = [] # å­˜å‚¨ (path_str, name)
        valid_input = True
        
        if not selected_indices:
            print("âš ï¸ è¾“å…¥ä¸ºç©ºï¼Œè¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªã€‚")
            continue
            
        try:
            for s_idx in selected_indices:
                idx = int(s_idx)
                if 1 <= idx <= len(config_paths):
                    # ç´¢å¼•æ˜¯ä»1å¼€å§‹çš„ï¼Œåˆ—è¡¨æ˜¯ä»0å¼€å§‹çš„
                    selected_path = config_paths[idx - 1]
                    # å­˜å‚¨é…ç½®çš„å®Œæ•´è·¯å¾„å­—ç¬¦ä¸²å’Œæ–‡ä»¶å
                    experiments_to_run.append((str(selected_path), selected_path.name))
                else:
                    print(f"âŒ åºå· '{idx}' è¶…å‡ºèŒƒå›´ (å¿…é¡»åœ¨ 1 åˆ° {len(config_paths)} ä¹‹é—´)ã€‚")
                    valid_input = False
                    break # åœæ­¢è§£æè¿™ä¸€æ‰¹è¾“å…¥
        except ValueError:
            print(f"âŒ è¾“å…¥æ— æ•ˆ: '{s_idx}' ä¸æ˜¯ä¸€ä¸ªæ•°å­—ã€‚")
            valid_input = False
        
        if valid_input:
            # ç¡®è®¤é€‰æ‹©
            print("\nä½ å·²é€‰æ‹©æŒ‰ä»¥ä¸‹é¡ºåºæ‰§è¡Œï¼š")
            for i, (_, name) in enumerate(experiments_to_run, 1):
                print(f"  {i}. {name}")
            confirm = input("ç¡®è®¤æ‰§è¡Œï¼Ÿ (y/n): ").strip().lower()
            if confirm == 'y':
                break # è¾“å…¥æœ‰æ•ˆä¸”å·²ç¡®è®¤ï¼Œè·³å‡º 'while True' å¾ªç¯
            else:
                print("ğŸ”„ å·²å–æ¶ˆï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
        else:
            print("ğŸ”„ è¯·é‡æ–°è¾“å…¥ã€‚")
            
    # 4. æŒ‰é¡ºåºæ‰§è¡Œé€‰ä¸­çš„å®éªŒ
    print(f"\nâœ¨ å‡†å¤‡æŒ‰é¡ºåºæ‰§è¡Œ {len(experiments_to_run)} ä¸ªå®éªŒ...")
    for i, (path_str, name) in enumerate(experiments_to_run, 1):
        print(f"\n--- é˜Ÿåˆ—: {i} / {len(experiments_to_run)} ---")
        # è°ƒç”¨æˆ‘ä»¬é‡æ„çš„å‡½æ•°
        run_experiment(path_str)
        
    print("\nğŸ‰ æ‰€æœ‰é€‰å®šçš„å®éªŒå‡å·²æ‰§è¡Œå®Œæ¯•ï¼")


if __name__ == '__main__':
    main() # è¿è¡Œæ–°çš„å¯åŠ¨å™¨