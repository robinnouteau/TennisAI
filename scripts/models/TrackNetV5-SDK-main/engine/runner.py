import torch
from tqdm import tqdm
from pathlib import Path
import numpy as np

class Runner:
    """
    ä¸€ä¸ªå°è£…äº†å®Œæ•´è®­ç»ƒ/éªŒè¯å¾ªç¯çš„æ‰§è¡Œå™¨ã€‚
    å®ƒè´Ÿè´£ç®¡ç†è®­ç»ƒçŠ¶æ€ã€æ‰§è¡Œè®­ç»ƒå¾ªç¯ã€è°ƒç”¨é’©å­ã€è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ¨¡å‹ã€‚
    """
    def __init__(self, model, optimizer, criterion, metric,
                 train_loader, val_loader, lr_scheduler, 
                 hooks, cfg):
        
        # --- æ ¸å¿ƒç»„ä»¶ ---
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.metric = metric
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.lr_scheduler = lr_scheduler
        self.hooks = hooks
        self.cfg = cfg
        
        # --- ç¯å¢ƒä¸è·¯å¾„ ---
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)
        self.work_dir = Path(cfg.work_dir)
        self.work_dir.mkdir(parents=True, exist_ok=True) # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
        
        # --- è®­ç»ƒè¿‡ç¨‹ä¸­çš„çŠ¶æ€å˜é‡ ---
        self.epoch = 0
        self.global_iter = 0
        self.inner_iter = 0
        self.max_epochs = cfg.total_epochs
        
        if hasattr(cfg, 'steps_per_epoch'):
            self.max_iters_per_epoch = cfg.steps_per_epoch
        else:
            # åªæœ‰åœ¨ train_loader ä¸ä¸º None æ—¶æ‰è·å–é•¿åº¦ï¼Œå¦åˆ™è®¾ä¸º 0ï¼ˆæµ‹è¯•æ¨¡å¼å¸¸è§æƒ…å†µï¼‰
            self.max_iters_per_epoch = len(self.train_loader) if self.train_loader is not None else 0

        self.outputs = {}      # ç”¨äºåœ¨é’©å­ä¹‹é—´ä¼ é€’ä¸´æ—¶æ•°æ® (å¦‚loss, metrics)
        self.best_metric = 0.0 # ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹çš„åˆ¤æ–­ä¾æ®

    def call_hooks(self, event_name):
        """è°ƒç”¨æ‰€æœ‰é’©å­ä¸­åä¸º event_name çš„æ–¹æ³•ã€‚"""
        for hook in self.hooks:
            getattr(hook, event_name)(self)

    def train_epoch(self):
        """æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ epochã€‚"""
        self.model.train()
        progress_bar = tqdm(self.train_loader, total=self.max_iters_per_epoch, 
                            desc=f"Train Epoch {self.epoch + 1}/{self.max_epochs}")

        base_lr = self.optimizer.param_groups[0]['lr']
                            
        for i, data_batch in enumerate(progress_bar):
            if i >= self.max_iters_per_epoch:
                break

            self.inner_iter = i
            self.call_hooks('before_iter')
            
            inputs = data_batch['image'].to(self.device)
            targets = data_batch['target'].to(self.device)
            
            self.optimizer.zero_grad()
            logits = self.model(inputs)
            loss = self.criterion(logits, targets, epoch_num=self.epoch+1)
            loss.backward()

            # âœ¨âœ¨âœ¨ ã€ã€ã€ ç´§æ€¥æ·»åŠ ï¼šæ‰‹åŠ¨ Warmup é€»è¾‘ ã€‘ã€‘ã€‘ âœ¨âœ¨âœ¨
            # 1. æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦å¯ç”¨äº† Warmup
            warmup_enabled = hasattr(self.cfg, 'lr_config') and \
                             self.cfg.lr_config is not None and \
                             self.cfg.lr_config.get('warmup') == 'linear'
                             
            if warmup_enabled:
                # 2. ä»é…ç½®è·å– Warmup å‚æ•°
                warmup_iters = self.cfg.lr_config.get('warmup_iters', 0) # æ€» Warmup è¿­ä»£æ¬¡æ•°
                warmup_ratio = self.cfg.lr_config.get('warmup_ratio', 1e-6) # åˆå§‹ LR æ¯”ä¾‹
                
                # 3. æ£€æŸ¥å½“å‰æ˜¯å¦å¤„äº Warmup é˜¶æ®µ
                if self.global_iter < warmup_iters:
                    # 4. è®¡ç®—å½“å‰çš„ Warmup å­¦ä¹ ç‡
                    #    k æ˜¯ä¸€ä¸ªä» warmup_ratio çº¿æ€§å¢é•¿åˆ° 1.0 çš„å› å­
                    k = (1 - warmup_ratio) * self.global_iter / warmup_iters + warmup_ratio
                    current_warmup_lr = base_lr * k
                    
                    # 5. æ‰‹åŠ¨è®¾ç½®ä¼˜åŒ–å™¨ä¸­æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = current_warmup_lr
                
                # 6. (å¯é€‰) Warmup ç»“æŸæ—¶ï¼Œç¡®ä¿æ¢å¤åˆ°åŸºç¡€å­¦ä¹ ç‡
                elif self.global_iter == warmup_iters:
                     for param_group in self.optimizer.param_groups:
                        param_group['lr'] = base_lr # æ¢å¤åŸºç¡€ LRï¼Œé˜²æ­¢æµ®ç‚¹è¯¯å·®
            # âœ¨âœ¨âœ¨ ã€ã€ã€ Warmup é€»è¾‘ç»“æŸ ã€‘ã€‘ã€‘ âœ¨âœ¨âœ¨

            # âœ¨âœ¨âœ¨ ã€ã€ã€ ç´§æ€¥æ·»åŠ ï¼šæ‰‹åŠ¨æ¢¯åº¦è£å‰ª ã€‘ã€‘ã€‘ âœ¨âœ¨âœ¨
            # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ optimizer_config å’Œ grad_clip è®¾ç½®
            if hasattr(self.cfg, 'optimizer_config') and \
               self.cfg.optimizer_config is not None and \
               'grad_clip' in self.cfg.optimizer_config and \
               self.cfg.optimizer_config['grad_clip'] is not None:
                
                # 2. ä»é…ç½®ä¸­è·å– max_norm å€¼
                max_norm = self.cfg.optimizer_config['grad_clip'].get('max_norm')
                
                # 3. å¦‚æœ max_norm æœ‰æ•ˆï¼Œåˆ™æ‰§è¡Œæ¢¯åº¦è£å‰ª
                if max_norm is not None and max_norm > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_norm)
                    # (å¯é€‰) å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸€è¡Œ print ç¡®è®¤è£å‰ªå·²æ‰§è¡Œ
                    # print(f"  -- Grad clipped with max_norm={max_norm}")
            # âœ¨âœ¨âœ¨ ã€ã€ã€ æ¢¯åº¦è£å‰ªä»£ç ç»“æŸ ã€‘ã€‘ã€‘ âœ¨âœ¨âœ¨

            self.optimizer.step()
            
            self.global_iter += 1
            self.outputs['loss'] = loss.item()
            self.outputs['batch_size'] = inputs.size(0)
            self.current_lr = self.optimizer.param_groups[0]['lr']

            self.call_hooks('after_iter')
            progress_bar.set_postfix(loss=loss.item())

    @torch.no_grad()
    def validate_epoch(self):
        """
        æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„éªŒè¯ epochï¼Œå¹¶æ­£ç¡®åœ°è°ƒç”¨é’©å­ï¼Œä¸ºå¯è§†åŒ–æä¾›æ”¯æŒã€‚
        """
        self.model.eval()
        self.metric.reset()
        
        # 1. æ–°å¢ï¼šå¹¿æ’­â€œéªŒè¯epochå¼€å§‹â€äº‹ä»¶
        # è¿™ä¼šè§¦å‘ ValidationVisualizerHook çš„ before_val_epoch æ–¹æ³•
        self.call_hooks('before_val_epoch')

        val_losses = []
        progress_bar = tqdm(self.val_loader, desc=f"Validate Epoch {self.epoch + 1}")
        for i, data_batch in enumerate(progress_bar):
            self.inner_iter = i
            
            inputs = data_batch['image'].to(self.device)
            targets = data_batch['target'].to(self.device)
            logits = self.model(inputs)
            
            # 2. æ–°å¢ï¼šå°†å½“å‰æ‰¹æ¬¡çš„æ•°æ®æš‚å­˜åˆ° outputs ä¸­ï¼Œä¾›é’©å­è®¿é—®
            self.outputs['val_batch'] = data_batch
            self.outputs['val_logits'] = logits

            # è®¡ç®—æŸå¤±å’ŒæŒ‡æ ‡
            loss = self.criterion(logits, targets)
            val_losses.append(loss.item())
            self.metric.update(logits, data_batch) # metricä¼ å…¥çš„æ˜¯logitså’Œå¯¹åº”çš„batchï¼Œç®—å‡ºæ˜¯tpè¿˜æ˜¯å•¥åˆ«çš„
            
            # 3. æ–°å¢ï¼šå¹¿æ’­â€œéªŒè¯iterç»“æŸâ€äº‹ä»¶
            # è¿™æ˜¯ ValidationVisualizerHook å·¥ä½œçš„å…³é”®ï¼
            self.call_hooks('after_val_iter')

        # è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆç»“æœ
        eval_results = self.metric.compute() # è¿™é‡Œæ‰ç®—å‡ºæ¥F1
        eval_results['loss'] = np.mean(val_losses)
        self.outputs['val_metrics'] = eval_results 
        print(f"Validation Results: {eval_results}")
        
        # 4. æ–°å¢ï¼šå¹¿æ’­â€œéªŒè¯epochç»“æŸâ€äº‹ä»¶
        self.call_hooks('after_val_epoch')

    def run(self):
        """å¯åŠ¨å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚"""
        print("ğŸš€ Starting Runner...")
        self.call_hooks('before_run')
        
        for self.epoch in range(self.max_epochs):
            self.call_hooks('before_epoch')
            self.train_epoch()
            
            if (self.epoch + 1) % self.cfg.evaluation['interval'] == 0:
                self.validate_epoch()
                
                # --- æ–°å¢ä»£ç  START ---
                # 1. æ¯æ¬¡éªŒè¯åï¼Œéƒ½ä¿å­˜å½“å‰ epoch çš„æ¨¡å‹å¿«ç…§
                # ä½¿ç”¨ f-string åˆ›å»ºä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„æ–‡ä»¶åï¼Œå¦‚ 'epoch_5.pth'
                checkpoint_path = self.work_dir / f'epoch_{self.epoch + 1}.pth'
                torch.save(self.model.state_dict(), checkpoint_path)
                print(f"âœ… Checkpoint saved for epoch {self.epoch + 1} to {checkpoint_path}")
                # --- æ–°å¢ä»£ç  END ---

                # 2. ä¿ç•™åŸæœ‰çš„é€»è¾‘ï¼Œç”¨äºä¿å­˜å’Œæ›´æ–°æ€§èƒ½æœ€ä½³çš„æ¨¡å‹
                current_f1 = self.outputs.get('val_metrics', {}).get('F1-Score', 0.0)
                if current_f1 > self.best_metric:
                    self.best_metric = current_f1
                    best_model_path = self.work_dir / 'best_model.pth'
                    torch.save(self.model.state_dict(), best_model_path)
                    print(f"ğŸ† New best model saved to {best_model_path} with F1-score: {self.best_metric:.4f}")
            
            # åªæœ‰åœ¨å­¦ä¹ ç‡è°ƒåº¦å™¨å­˜åœ¨æ—¶ï¼Œæ‰æ‰§è¡Œ .step()
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()

            self.call_hooks('after_epoch')
            
        self.call_hooks('after_run')
        print("\nğŸ‰ Training finished!")
        print(f"Best F1-Score on validation set: {self.best_metric:.4f}")


    # ... æ‚¨åŸæœ‰çš„ä»£ç  ...

    @torch.no_grad()
    def test(self, test_loader=None, checkpoint_path=None):
        """
        æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹ã€‚
        å‚æ•°:
            test_loader: æŒ‡å®šæµ‹è¯•æ•°æ®é›†åŠ è½½å™¨ã€‚è‹¥ä¸º None åˆ™å°è¯•ä½¿ç”¨ val_loaderã€‚
            checkpoint_path: æŒ‡å®šè¦åŠ è½½çš„æƒé‡è·¯å¾„ (.pth)ã€‚
        """
        print(f"ğŸ” Starting Testing Mode...")
        
        # 1. åŠ è½½æŒ‡å®šçš„æƒé‡ (å¦‚æœæ˜¯ä» test.py å¯åŠ¨)
        if checkpoint_path is not None:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint)
            print(f"âœ… Loaded checkpoint from {checkpoint_path}")

        self.model.eval()
        self.metric.reset()
        
        # ä½¿ç”¨ä¼ å…¥çš„ loader æˆ–é»˜è®¤çš„ val_loader
        loader = test_loader if test_loader is not None else self.val_loader
        
        # 2. è§¦å‘æµ‹è¯•ä¸“ç”¨çš„é’©å­ (å»ºè®®åœ¨ BaseHook ä¸­ä¹Ÿå¢åŠ å¯¹åº”çš„æ¥å£)
        self.call_hooks('before_test_epoch')

        test_losses = []
        progress_bar = tqdm(loader, desc="Testing")
        
        for i, data_batch in enumerate(progress_bar):
            self.inner_iter = i
            
            inputs = data_batch['image'].to(self.device)
            targets = data_batch['target'].to(self.device)
            logits = self.model(inputs)
            
            # å°†æ•°æ®å­˜å…¥ outputs ä¾› VisualizerHook ç­‰ä½¿ç”¨
            self.outputs['test_batch'] = data_batch
            self.outputs['test_logits'] = logits

            # è®¡ç®—
            loss = self.criterion(logits, targets)
            test_losses.append(loss.item())
            self.metric.update(logits, data_batch)
            
            # 3. è§¦å‘æµ‹è¯•è¿­ä»£é’©å­
            self.call_hooks('after_test_iter')

        # 4. æ±‡æ€»ç»“æœ
        eval_results = self.metric.compute()
        eval_results['loss'] = np.mean(test_losses)
        self.outputs['test_metrics'] = eval_results
        
        print(f"\n" + "="*30)
        print(f" Final Test Results: ")
        for k, v in eval_results.items():
            print(f"  - {k}: {v:.4f}" if isinstance(v, float) else f"  - {k}: {v}")
        print("="*30 + "\n")
        
        self.call_hooks('after_test_epoch')
        return eval_results